{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CitiBike ML Demo\n",
    "By: David Stearns  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ alt text for screen readers](sf_ds.png \"Text to show on mouseover\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ alt text for screen readers](sf_ds2.png \"Text to show on mouseover\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ alt text for screen readers](sf_ds3.png \"Text to show on mouseover\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ alt text for screen readers](sf_ds4.png \"Text to show on mouseover\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ alt text for screen readers](sf_ds5.png \"Text to show on mouseover\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ alt text for screen readers](sf_ds6.png \"Text to show on mouseover\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.snowpark\n",
    "from snowflake.snowpark.functions import sproc\n",
    "from snowflake.snowpark.functions import udf\n",
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark import types as T\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import sys\n",
    "import math\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark import Window\n",
    "pd.set_option('display.max_columns', None)\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import credentials\n",
    "import io\n",
    "import joblib\n",
    "import cachetools\n",
    "#\n",
    "from multiprocessing import pool\n",
    "#\n",
    "conn = {\n",
    "    \"account\": credentials.credentials[\"account\"],\n",
    "    \"user\": credentials.credentials[\"username\"],\n",
    "    \"password\": credentials.credentials[\"password\"],\n",
    "    \"role\": credentials.credentials[\"role\"],\n",
    "    \"warehouse\": credentials.credentials[\"warehouse\"],\n",
    "    \"database\": credentials.credentials[\"database\"],\n",
    "    \"schema\": credentials.credentials[\"schema\"]\n",
    "}\n",
    "session = Session.builder.configs(conn).create()\n",
    "import snowflake.snowpark\n",
    "from snowflake.snowpark.functions import sproc\n",
    "from snowflake.snowpark.functions import udf\n",
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark import types as T\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import sys\n",
    "import math\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark import Window\n",
    "pd.set_option('display.max_columns', None)\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import credentials\n",
    "import io\n",
    "import joblib\n",
    "import cachetools\n",
    "import time\n",
    "#\n",
    "from multiprocessing import pool\n",
    "#\n",
    "conn = {\n",
    "    \"account\": credentials.credentials[\"account\"],\n",
    "    \"user\": credentials.credentials[\"username\"],\n",
    "    \"password\": credentials.credentials[\"password\"],\n",
    "    \"role\": credentials.credentials[\"role\"],\n",
    "    \"warehouse\": credentials.credentials[\"warehouse\"],\n",
    "    \"database\": credentials.credentials[\"database\"],\n",
    "    \"schema\": credentials.credentials[\"schema\"]\n",
    "}\n",
    "session = Session.builder.configs(conn).create()\n",
    "print(\"-------------------------------------------\")\n",
    "warehouse = session.sql(\"select current_warehouse()\").collect()[0][0]\n",
    "print(f\"\"\"Using {warehouse} - a high memory warehouse\"\"\")\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"Preparing the warehouse metadata:\")\n",
    "print(\"-------------------------------------------\")\n",
    "session.sql(\"show warehouses like 'high_mem_wh'\").collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(\"select current_warehouse()\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(\"use warehouse snowpark\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(session, model, path):\n",
    "  input_stream = io.BytesIO()\n",
    "  joblib.dump(model, input_stream)\n",
    "  session._conn._cursor.upload_stream(input_stream, path)\n",
    "  return \"successfully created file: \" + path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(f\"create or replace stage citibike_ml\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips_hourly = session.table(\"trips_stations_vw\")\n",
    "print(\"Aggregating TRIPS data\")\n",
    "print(\"-------------------------------------------\")\n",
    "df_trips_daily = df_trips_hourly.select(F.to_date(F.col(\"STARTTIME\")).as_(\"ds\")).groupBy(F.col(\"ds\")).count().select(F.col(\"ds\"), F.col(\"COUNT\").as_(\"y\"))\n",
    "df_weather = session.table(\"weather_vw\")\n",
    "print(\"Joining the aggregated TRIPS data with WEATHER data from the Snowflake Data Marketplace\")\n",
    "print(\"-------------------------------------------\")\n",
    "df_trips_weather = df_trips_daily.join(df_weather, df_trips_daily[\"DS\"] == df_weather[\"OBSERVATION_DATE\"])\\\n",
    "                    .select(F.col(\"DS\"),F.col(\"Y\"), F.col(\"TEMP_AVG_C\"), F.col(\"TOT_PRECIP_IN\"))\n",
    "print(\"Feature engineering: creating an indicator column for rain (1 = rain, 0 = no rain)\")\n",
    "print(\"-------------------------------------------\")\n",
    "df_trips_weather = df_trips_weather.withColumn(\"rain_indicator\", F.when(F.col(\"TOT_PRECIP_IN\") > 0, F.lit(1) ).otherwise(F.lit(0)))\n",
    "print(\"Sorting the table by date\")\n",
    "print(\"-------------------------------------------\")\n",
    "df_trips_weather = df_trips_weather.sort(F.col(\"DS\"))\n",
    "print(\"Writing back the table to the Snowflake database with selected columns\")\n",
    "print(\"-------------------------------------------\")\n",
    "df_trips_weather.select(\"DS\", \"Y\", \"RAIN_INDICATOR\", \"TEMP_AVG_C\").write.mode(\"overwrite\").save_as_table(\"citibike_ml_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_ml = session.table(\"citibike_ml_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature engineering: creating a one day shift and natural log transformation\",\"\\n\",\n",
    "        \"of the TRIPS figure (target) and saving it back to the Snowflake database\",\"\\n\",\n",
    "        \"to create a features table\")\n",
    "print(\"-------------------------------------------\")\n",
    "session.sql(\"\"\" select\n",
    "                    RAIN_INDICATOR,\n",
    "                    TEMP_AVG_C,\n",
    "                    Y_log,\n",
    "                    LAG(Y_log,1,0) OVER (order by DS) as Y_yesterday\n",
    "                from (\n",
    "                    select ds, rain_indicator, temp_avg_c, round(ln(Y), 2) as Y_log from citibike_ml_demo\n",
    "                ) A\"\"\").write.mode(\"overwrite\").save_as_table(\"citibike_ml_demo_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.table(\"citibike_ml_demo_features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_ml = session.table(\"citibike_ml_demo\")\n",
    "cb_ml.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating function to be registered as a SPROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.add_packages('snowflake-snowpark-python', 'scikit-learn', 'pandas', 'numpy', 'joblib', 'cachetools')\n",
    "def citibike_ml_demo(session: snowflake.snowpark.Session) -> str:\n",
    "\n",
    "    import snowflake.snowpark as snp\n",
    "    from snowflake.snowpark import functions as F\n",
    "    from snowflake.snowpark import types as T\n",
    "\n",
    "    model_name = \"predict_trips\"\n",
    "    stage_name = \"citibike_ml\"\n",
    "    df = session.table(\"citibike_ml_demo_features\")\n",
    "    pandas_df = df.toPandas()\n",
    "    pandas_df = pandas_df[1:]\n",
    "\n",
    "    X = pandas_df[[\"RAIN_INDICATOR\", \"TEMP_AVG_C\", \"Y_YESTERDAY\"]]\n",
    "    y = pandas_df[[\"Y_LOG\"]]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33 )\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42, min_samples_split=3)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    save_model(session, rf, f\"@{stage_name}/{model_name}.joblib\")\n",
    "\n",
    "    model_name_full = session.sql(f\"ls @{stage_name}\").collect()[0][0]\n",
    "\n",
    "    performance = cross_validate(rf, X, y, cv=3)\n",
    "\n",
    "    return f\"Created model: {model_name_full}.... Performance Metrics: {performance}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Registering the SPROC with the function I just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registering my SPROC within Snowflake\n",
    "print(\"Registering the SPROC in Snowflake as a Permanent SPROC with dedicated location and name\")\n",
    "train_rfc_model = session.sproc.register(citibike_ml_demo, replace=True,\n",
    "                                         is_permanent=True, name=\"citibike_train\",\n",
    "                                        stage_location=\"@CITIBIKE_ML\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the SPROC I just created to perform model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the SPROC for model training\n",
    "train_rfc_model(session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing my model into the session\n",
    "session.add_import(\"@citibike_ml/predict_trips.joblib\")  \n",
    "\n",
    "# Caching my model so it is only read once\n",
    "@cachetools.cached(cache={})\n",
    "\n",
    "# Function to read the model file into another function\n",
    "def read_file(filename):\n",
    "       import os\n",
    "       import_dir = sys._xoptions.get(\"snowflake_import_directory\")\n",
    "       if import_dir:\n",
    "              with open(os.path.join(import_dir, filename), 'rb') as file:\n",
    "                     m = joblib.load(file)\n",
    "                     return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Registering a UDF with a Vectorized function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registering the UDF\n",
    "@udf(session=session, name=\"predict_trips\", is_permanent=True, stage_location=\"@citibike_ml\", replace=True)\n",
    "def predict_trips(df: T.PandasDataFrame[int, float, float]) -> T.PandasSeries[float]:\n",
    "    m = read_file('predict_trips.joblib')\n",
    "    if isinstance(m, str):\n",
    "        return m\n",
    "    return m.predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = session.sql(\"select rain_indicator, temp_avg_c, y_yesterday from citibike_ml_demo_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = test_df.select(\n",
    "    *test_df,\n",
    "    F.ceil(F.call_udf(\"predict_trips\", *test_df)).alias('PREDICTION')\n",
    "    ).where(F.col(\"Y_YESTERDAY\") != 0).select(\n",
    "        *test_df,\n",
    "        F.col(\"PREDICTION\"),\n",
    "        F.when(\n",
    "            F.col(\"PREDICTION\") != F.col(\"Y_YESTERDAY\"),\n",
    "            F.round(((F.col(\"PREDICTION\")-F.col(\"Y_YESTERDAY\"))/F.col(\"Y_YESTERDAY\"))*100, 2)\n",
    "        ).alias(\"ERROR_PERCENT\")\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = str(datetime.datetime.today().date())\n",
    "date = date.replace(\"-\", \"\")\n",
    "result_table = f\"results_{date}\"\n",
    "results.write.save_as_table(table_name=result_table, mode=\"overwrite\")\n",
    "print(\"-----------------------------\")\n",
    "print(f\"\"\"Created table {result_table}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(f\"select * from {result_table}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('snowpark_demos')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61aef635fca093b00ca00b2731ed2b6a94ed91192af7cb2d724f77a96c850456"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
