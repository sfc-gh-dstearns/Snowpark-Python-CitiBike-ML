{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CitiBike ML Demo\n",
    "By: David Stearns  \n",
    "  \n",
    "This demo (while fully functional *could use a bit of clean up!* :D ) demonstrates Snowpark Python's capability to build, train, and deploy machine learning models. It uses the Scalar UDF function which is great for single predictions, however, if you would like to do full table predictions or use input batches, the best way to accomplish that is to use Vectorized UDF's. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.snowpark\n",
    "from snowflake.snowpark.functions import sproc\n",
    "from snowflake.snowpark.functions import udf\n",
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark import types as T\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import sys\n",
    "import math\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark import Window\n",
    "pd.set_option('display.max_columns', None)\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import credentials\n",
    "import io\n",
    "import joblib\n",
    "import cachetools\n",
    "#\n",
    "from multiprocessing import pool\n",
    "#\n",
    "conn = {\n",
    "    \"account\": credentials.credentials[\"account\"],\n",
    "    \"user\": credentials.credentials[\"username\"],\n",
    "    \"password\": credentials.credentials[\"password\"],\n",
    "    \"role\": credentials.credentials[\"role\"],\n",
    "    \"warehouse\": credentials.credentials[\"warehouse\"],\n",
    "    \"database\": credentials.credentials[\"database\"],\n",
    "    \"schema\": credentials.credentials[\"schema\"]\n",
    "}\n",
    "session = Session.builder.configs(conn).create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(session, model, path):\n",
    "  input_stream = io.BytesIO()\n",
    "  joblib.dump(model, input_stream)\n",
    "  session._conn._cursor.upload_stream(input_stream, path)\n",
    "  return \"successfully created file: \" + path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='Stage area CITIBIKE_ML successfully created.')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql(f\"create or replace stage citibike_ml\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips_hourly = session.table(\"trips_stations_vw\")\n",
    "print(\"Aggregating TRIPS data\")\n",
    "print(\"-------------------------------------------\")\n",
    "df_trips_daily = df_trips_hourly.select(F.to_date(F.col(\"STARTTIME\")).as_(\"ds\")).groupBy(F.col(\"ds\")).count().select(F.col(\"ds\"), F.col(\"COUNT\").as_(\"y\"))\n",
    "df_weather = session.table(\"weather_vw\")\n",
    "print(\"Joining the aggregated TRIPS data with WEATHER data from the Snowflake Data Marketplace\")\n",
    "print(\"-------------------------------------------\")\n",
    "df_trips_weather = df_trips_daily.join(df_weather, df_trips_daily[\"DS\"] == df_weather[\"OBSERVATION_DATE\"])\\\n",
    "                    .select(F.col(\"DS\"),F.col(\"Y\"), F.col(\"TEMP_AVG_C\"), F.col(\"TOT_PRECIP_IN\"))\n",
    "print(\"Feature engineering: creating an indicator column for rain (1 = rain, 0 = no rain)\")\n",
    "print(\"-------------------------------------------\")\n",
    "df_trips_weather = df_trips_weather.withColumn(\"rain_indicator\", F.when(F.col(\"TOT_PRECIP_IN\") > 0, F.lit(1) ).otherwise(F.lit(0)))\n",
    "print(\"Sorting the table by date\")\n",
    "print(\"-------------------------------------------\")\n",
    "df_trips_weather = df_trips_weather.sort(F.col(\"DS\"))\n",
    "print(\"Writing back the table to the Snowflake database with selected columns\")\n",
    "print(\"-------------------------------------------\")\n",
    "df_trips_weather.select(\"DS\", \"Y\", \"RAIN_INDICATOR\", \"TEMP_AVG_C\").write.mode(\"overwrite\").save_as_table(\"citibike_ml_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_ml = session.table(\"citibike_ml_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature engineering: creating a one day shift and natural log transformation\",\"\\n\",\n",
    "        \"of the TRIPS figure (target) and saving it back to the Snowflake database\",\"\\n\",\n",
    "        \"to create a features table\")\n",
    "print(\"-------------------------------------------\")\n",
    "session.sql(\"\"\" select\n",
    "                    RAIN_INDICATOR,\n",
    "                    TEMP_AVG_C,\n",
    "                    Y_log,\n",
    "                    LAG(Y_log,1,0) OVER (order by DS) as Y_yesterday\n",
    "                from (\n",
    "                    select ds, rain_indicator, temp_avg_c, round(ln(Y), 2) as Y_log from citibike_ml_demo\n",
    "                ) A\"\"\").write.mode(\"overwrite\").save_as_table(\"citibike_ml_demo_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.table(\"citibike_ml_demo_features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next line of code shows feature engineering happening inside of this Stored Procedure in a Pandas dataframe. I would actually recommend against this (I just did this in a time crunch). The best way to do this is to create a stored procedure using a standard warehouse to run all of your feature transformations NOT using Pandas (so, SQL or just using the Snowpark Dataframe API syntax), and save that table. Once you have your final table, you can then create a Stored Procedure using a High Memory Warehouse to run the model training flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------\n",
      "|\"DS\"        |\"Y\"    |\"RAIN_INDICATOR\"  |\"TEMP_AVG_C\"        |\n",
      "--------------------------------------------------------------\n",
      "|2020-09-24  |21155  |0                 |20.064              |\n",
      "|2020-09-25  |20186  |0                 |20.116              |\n",
      "|2020-09-26  |20306  |1                 |20.488              |\n",
      "|2020-09-27  |18125  |1                 |22.362000000000002  |\n",
      "|2020-09-28  |16108  |1                 |22.259999999999998  |\n",
      "--------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cb_ml = session.table(\"citibike_ml_demo\")\n",
    "cb_ml.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.add_packages('snowflake-snowpark-python', 'scikit-learn', 'pandas', 'numpy', 'joblib', 'cachetools')\n",
    "def citibike_ml_demo(session: snowflake.snowpark.Session) -> str:\n",
    "\n",
    "    import snowflake.snowpark as snp\n",
    "    from snowflake.snowpark import functions as F\n",
    "    from snowflake.snowpark import types as T\n",
    "\n",
    "    model_name = \"predict_trips\"\n",
    "    stage_name = \"citibike_ml\"\n",
    "    df = session.table(\"citibike_ml_demo_features\")\n",
    "    pandas_df = df.toPandas()\n",
    "    pandas_df = pandas_df[1:]\n",
    "\n",
    "    X = pandas_df[[\"RAIN_INDICATOR\", \"TEMP_AVG_C\", \"Y_YESTERDAY\"]]\n",
    "    y = pandas_df[[\"Y_LOG\"]]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33 )\n",
    "    rf = RandomForestRegressor(n_estimators=100)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    save_model(session, rf, f\"@{stage_name}/{model_name}.joblib\")\n",
    "\n",
    "    model_name_full = session.sql(f\"ls @{stage_name}\").collect()[0][0]\n",
    "\n",
    "    performance = cross_validate(rf, X, y, cv=3)\n",
    "\n",
    "    return f\"Created model: {model_name_full}.... Performance Metrics: {performance}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registering my SPROC within Snowflake\n",
    "print(\"Registering the SPROC in Snowflake as a Permanent SPROC with dedicated location and name\")\n",
    "train_rfc_model = session.sproc.register(citibike_ml_demo, replace=True,\n",
    "                                         is_permanent=True, name=\"citibike_train\",\n",
    "                                        stage_location=\"@CITIBIKE_ML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Created model: citibike_ml/predict_trips.joblib.... Performance Metrics: {'fit_time': array([0.13583279, 0.13426924, 0.13235784]), 'score_time': array([0.01061487, 0.00998688, 0.01009941]), 'test_score': array([0.95149652, 0.9094952 , 0.93581312])}\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running the SPROC for model training\n",
    "train_rfc_model(session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing my model into the session\n",
    "session.add_import(\"@citibike_ml/predict_trips.joblib\")  \n",
    "\n",
    "# Caching my model so it is only read once\n",
    "@cachetools.cached(cache={})\n",
    "\n",
    "# Function to read the model file into another function\n",
    "def read_file(filename):\n",
    "       import os\n",
    "       import_dir = sys._xoptions.get(\"snowflake_import_directory\")\n",
    "       if import_dir:\n",
    "              with open(os.path.join(import_dir, filename), 'rb') as file:\n",
    "                     m = joblib.load(file)\n",
    "                     return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registering the UDF\n",
    "@udf(session=session, name=\"predict_trips\", is_permanent=True, stage_location=\"@citibike_ml\", replace=True)\n",
    "def predict_trips(df: T.PandasDataFrame[int, float, float]) -> T.PandasSeries[float]:\n",
    "    m = read_file('predict_trips.joblib')\n",
    "    if isinstance(m, str):\n",
    "        return m\n",
    "    return m.predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = session.sql(\"select rain_indicator, temp_avg_c, y_yesterday from citibike_ml_demo_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = test_df.select(\n",
    "    *test_df,\n",
    "    F.ceil(F.call_udf(\"predict_trips\", *test_df)).alias('PREDICTION')\n",
    "    ).where(F.col(\"Y_YESTERDAY\") != 0).select(\n",
    "        *test_df,\n",
    "        F.col(\"PREDICTION\"),\n",
    "        F.when(\n",
    "            F.col(\"PREDICTION\") != F.col(\"Y_YESTERDAY\"),\n",
    "            ((F.col(\"PREDICTION\")-F.col(\"Y_YESTERDAY\"))/F.col(\"Y_YESTERDAY\"))*100\n",
    "        ).alias(\"ERROR_PERCENT\")\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "|\"RAIN_INDICATOR\"  |\"TEMP_AVG_C\"  |\"Y_YESTERDAY\"  |\"PREDICTION\"  |\n",
      "------------------------------------------------------------------\n",
      "|1                 |27.52         |0.0            |9.0           |\n",
      "|1                 |24.996        |9.8            |10.0          |\n",
      "|0                 |26.174        |9.83           |10.0          |\n",
      "|0                 |24.226        |9.81           |10.0          |\n",
      "|1                 |20.406        |9.78           |10.0          |\n",
      "|1                 |21.924        |9.69           |10.0          |\n",
      "|1                 |23.54         |9.62           |10.0          |\n",
      "|1                 |21.45         |9.75           |10.0          |\n",
      "|0                 |22.274        |9.82           |10.0          |\n",
      "|0                 |24.94         |9.86           |10.0          |\n",
      "------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('snowpark_demos')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61aef635fca093b00ca00b2731ed2b6a94ed91192af7cb2d724f77a96c850456"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
